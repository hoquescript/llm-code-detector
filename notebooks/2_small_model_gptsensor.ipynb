{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06f7cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Set pandas to show full column content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Read data\n",
    "df = pd.read_json(\"../data/codegptsensor/python/train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35ce7f",
   "metadata": {},
   "source": [
    "##### Creating a small sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76d59b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# orient=\"records\": Each DataFrame row becomes a JSON object. Without lines=True, it outputs a JSON array of objects.\n",
    "# lines=True: Writes each record on its own line (JSONL), one JSON object per line.\n",
    "df_small.to_json(\"../data/codegptsensor/python/train_small.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd667477",
   "metadata": {},
   "source": [
    "##### Verifying the small dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d08b29db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>code</th>\n",
       "      <th>contrast</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gp130060</td>\n",
       "      <td>def save_file(filename, data, mk_parents=True):\\n    \"\"\"Save file to disk.\\n    Paramaters\\n    ----------\\n    filename : pathlib.Path\\n        Path to the file.\\n    data : str\\n        File contents.\\n    mk_parents : bool, optional\\n        If to create parent directories.\\n    \"\"\"\\n    parent = filename.parent\\n    if not parent.exists() and mk_parents:\\n        logger.debug(\"Creating directory: %s\", parent.as_posix())\\n        parent.mkdir(parents=True)\\n    with open(filename, mode=\"w\") as f:\\n        logger.debug(\"Saving file: %s\", filename.as_posix())\\n        f.write(data)</td>\n",
       "      <td>import pathlib\\n\\ndef save_to_disk(filename: pathlib.Path, data: str, mk_parents: bool = False) -&gt; None:\\n    if mk_parents:\\n        filename.parent.mkdir(parents=True, exist_ok=True)\\n    with open(filename, 'w') as f:\\n        f.write(data)\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gp191806</td>\n",
       "      <td>import functools\\nimport logging\\n\\ndef wrap_callbacks(callback_fn):\\n    @functools.wraps(callback_fn)\\n    def wrapper(*args, **kwargs):\\n        try:\\n            return callback_fn(*args, **kwargs)\\n        except Exception as e:\\n            logging.exception(f\"Error in callback: {e}\")\\n            return \"An error occurred in the callback\"\\n    return wrapper\\n</td>\n",
       "      <td>def dont_crash(fn):\\n    \"\"\"\\n    Wraps callbacks: a simple information is raised in place of a program crash.\\n    \"\"\"\\n    def safe_exec(self, *args, **kwargs):\\n        try:\\n            return fn(self, *args, **kwargs)\\n        except Exception as e:\\n            logging.exception(e)\\n            QMessageBox.information(\\n                self, type(e).__name__, \" \".join(str(x) for x in e.args)\\n            )\\n    return safe_exec</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gp166948</td>\n",
       "      <td>def normalizeGlyphUnicodes(value):\\n    \"\"\"\\n    Normalizes glyph unicodes.\\n    * **value** must be a ``list``.\\n    * **value** items must normalize as glyph unicodes with\\n      :func:`normalizeGlyphUnicode`.\\n    * **value** must not repeat unicode values.\\n    * Returned value will be a ``tuple`` of ints.\\n    \"\"\"\\n    if not isinstance(value, (tuple, list)):\\n        raise TypeError(\"Glyph unicodes must be a list, not %s.\"\\n                        % type(value).__name__)\\n    values = [normalizeGlyphUnicode(v) for v in value]\\n    duplicates = [v for v, count in Counter(value).items() if count &gt; 1]\\n    if len(duplicates) != 0:\\n        raise ValueError(\"Duplicate unicode values are not allowed.\")\\n    return tuple(values)</td>\n",
       "      <td>def normalize_glyph_unicodes(value):\\n    \"\"\"\\n    Normalizes glyph unicodes.\\n    * **value** must be a ``list``.\\n    * **value** items must normalize as glyph unicodes with\\n      :func:`normalizeGlyphUnicode`.\\n    * **value** must not repeat unicode values.\\n    * Returned value will be a ``tuple`` of ints.\\n    \"\"\"\\n    from fontTools.misc.transform import Transform\\n    glyphs = []\\n    for glyph in value:\\n        glyph_norm = normalizeGlyphUnicode(glyph)\\n        if glyph_norm not in glyphs:\\n            glyphs.append(glyph_norm)\\n    return tuple(map(ord, glyphs))\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  \\\n",
       "0  gp130060   \n",
       "1  gp191806   \n",
       "2  gp166948   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 code  \\\n",
       "0                                                                                                                                                       def save_file(filename, data, mk_parents=True):\\n    \"\"\"Save file to disk.\\n    Paramaters\\n    ----------\\n    filename : pathlib.Path\\n        Path to the file.\\n    data : str\\n        File contents.\\n    mk_parents : bool, optional\\n        If to create parent directories.\\n    \"\"\"\\n    parent = filename.parent\\n    if not parent.exists() and mk_parents:\\n        logger.debug(\"Creating directory: %s\", parent.as_posix())\\n        parent.mkdir(parents=True)\\n    with open(filename, mode=\"w\") as f:\\n        logger.debug(\"Saving file: %s\", filename.as_posix())\\n        f.write(data)   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                   import functools\\nimport logging\\n\\ndef wrap_callbacks(callback_fn):\\n    @functools.wraps(callback_fn)\\n    def wrapper(*args, **kwargs):\\n        try:\\n            return callback_fn(*args, **kwargs)\\n        except Exception as e:\\n            logging.exception(f\"Error in callback: {e}\")\\n            return \"An error occurred in the callback\"\\n    return wrapper\\n   \n",
       "2  def normalizeGlyphUnicodes(value):\\n    \"\"\"\\n    Normalizes glyph unicodes.\\n    * **value** must be a ``list``.\\n    * **value** items must normalize as glyph unicodes with\\n      :func:`normalizeGlyphUnicode`.\\n    * **value** must not repeat unicode values.\\n    * Returned value will be a ``tuple`` of ints.\\n    \"\"\"\\n    if not isinstance(value, (tuple, list)):\\n        raise TypeError(\"Glyph unicodes must be a list, not %s.\"\\n                        % type(value).__name__)\\n    values = [normalizeGlyphUnicode(v) for v in value]\\n    duplicates = [v for v, count in Counter(value).items() if count > 1]\\n    if len(duplicates) != 0:\\n        raise ValueError(\"Duplicate unicode values are not allowed.\")\\n    return tuple(values)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 contrast  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                   import pathlib\\n\\ndef save_to_disk(filename: pathlib.Path, data: str, mk_parents: bool = False) -> None:\\n    if mk_parents:\\n        filename.parent.mkdir(parents=True, exist_ok=True)\\n    with open(filename, 'w') as f:\\n        f.write(data)\\n   \n",
       "1                                                                                                                                                   def dont_crash(fn):\\n    \"\"\"\\n    Wraps callbacks: a simple information is raised in place of a program crash.\\n    \"\"\"\\n    def safe_exec(self, *args, **kwargs):\\n        try:\\n            return fn(self, *args, **kwargs)\\n        except Exception as e:\\n            logging.exception(e)\\n            QMessageBox.information(\\n                self, type(e).__name__, \" \".join(str(x) for x in e.args)\\n            )\\n    return safe_exec   \n",
       "2  def normalize_glyph_unicodes(value):\\n    \"\"\"\\n    Normalizes glyph unicodes.\\n    * **value** must be a ``list``.\\n    * **value** items must normalize as glyph unicodes with\\n      :func:`normalizeGlyphUnicode`.\\n    * **value** must not repeat unicode values.\\n    * Returned value will be a ``tuple`` of ints.\\n    \"\"\"\\n    from fontTools.misc.transform import Transform\\n    glyphs = []\\n    for glyph in value:\\n        glyph_norm = normalizeGlyphUnicode(glyph)\\n        if glyph_norm not in glyphs:\\n            glyphs.append(glyph_norm)\\n    return tuple(map(ord, glyphs))\\n   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = pd.read_json(\"../data/codegptsensor/python/train_small.jsonl\", lines=True)\n",
    "df_small.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57278b",
   "metadata": {},
   "source": [
    "##### Load UniXcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "689a0a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n",
      "Model type: <class 'transformers.models.roberta.modeling_roberta.RobertaModel'>\n",
      "Model size: 125.93M parameters\n"
     ]
    }
   ],
   "source": [
    "# A tokenizer converts code (text) into numbers that the model can process.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "# Unixcoder is a neural network that takes numbers and outputs predictions.\n",
    "model = AutoModel.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e973ce",
   "metadata": {},
   "source": [
    "##### Test with a simple code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa2f1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Tokenizer working!\n",
      "Input shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "test_code = \"def hello():\\n    print('Hello world')\"\n",
    "inputs = tokenizer(test_code, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "print(\"\\n✓ Tokenizer working!\")\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6484d",
   "metadata": {},
   "source": [
    "##### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d1fa10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model forward pass working!\n",
      "Embedding shape: torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "print(f\"✓ Model forward pass working!\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799b9dd",
   "metadata": {},
   "source": [
    "##### Input and output splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "947abeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000 (Human: 1000, AI: 1000)\n",
      "Label counts: {0: 1000, 1: 1000}\n"
     ]
    }
   ],
   "source": [
    "# Create data for both columns\n",
    "human_samples = []\n",
    "ai_samples = []\n",
    "for _, row in df_small.iterrows():\n",
    "    if row['label'] == 0:\n",
    "        human_samples.append(row['code'])\n",
    "        ai_samples.append(row['contrast'])\n",
    "    else:\n",
    "        ai_samples.append(row['code'])\n",
    "        human_samples.append(row['contrast'])\n",
    "\n",
    "# Create a label list (same length as the samples)\n",
    "samples = human_samples + ai_samples\n",
    "labels = [0]*len(human_samples) + [1]*len(ai_samples)\n",
    "\n",
    "print(f\"Total samples: {len(samples)} (Human: {len(human_samples)}, AI: {len(ai_samples)})\")\n",
    "print(f\"Label counts: {pd.Series(labels).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4fc1c",
   "metadata": {},
   "source": [
    "##### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d14ce12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:47<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2000, 768)\n",
      "y length: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "embeddings = []\n",
    "batch_size = 32  # Small batch for safety\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(samples), batch_size)):\n",
    "        batch_text = samples[i:i+batch_size]\n",
    "        # Tokenize and pad\n",
    "        inputs = tokenizer(batch_text, padding='max_length', truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        # Use the [CLS] token (first in the sequence) as embedding\n",
    "        batch_emb = outputs.last_hidden_state[:, 0, :].cpu()  # shape: [batch, 768]\n",
    "        embeddings.append(batch_emb)\n",
    "\n",
    "# Concatenate over all batches\n",
    "X = torch.cat(embeddings, dim=0).numpy()\n",
    "y = labels\n",
    "\n",
    "print(f\"X shape: {X.shape}\")  # Should be (2000, 768)\n",
    "print(f\"y length: {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6fa79f",
   "metadata": {},
   "source": [
    "##### Train a Simple Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0ffac",
   "metadata": {},
   "source": [
    "##### Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8def09c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeClassifier(\n",
      "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "Total parameters: 197378\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CodeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, num_classes=2):\n",
    "        super(CodeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "classifier = CodeClassifier()\n",
    "print(classifier)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in classifier.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dc648",
   "metadata": {},
   "source": [
    "## Paper Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59718a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 50\n",
      "Test batches: 13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3ec62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 50/50 [00:00<00:00, 752.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.4709, Accuracy = 76.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 50/50 [00:00<00:00, 1336.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.2929, Accuracy = 87.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 50/50 [00:00<00:00, 1231.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.2221, Accuracy = 91.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 50/50 [00:00<00:00, 1243.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 0.1635, Accuracy = 93.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 50/50 [00:00<00:00, 1311.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.1351, Accuracy = 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 50/50 [00:00<00:00, 1327.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.0902, Accuracy = 97.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 50/50 [00:00<00:00, 1379.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.0746, Accuracy = 97.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 50/50 [00:00<00:00, 1362.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 0.0542, Accuracy = 98.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 50/50 [00:00<00:00, 1233.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 0.0408, Accuracy = 99.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 50/50 [00:00<00:00, 1452.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.0453, Accuracy = 98.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()  # Standard loss for classification\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Forward pass\n",
    "        outputs = classifier(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}: Loss = {train_loss/len(train_loader):.4f}, Accuracy = {train_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "219d3bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 85.50%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.84      0.87      0.86       199\n",
      "          AI       0.87      0.84      0.85       201\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.86      0.86      0.85       400\n",
      "weighted avg       0.86      0.85      0.85       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = classifier(batch_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(batch_y.numpy())\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f\"\\nTest Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Detailed metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(all_labels, all_preds, target_names=['Human', 'AI']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08bceaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 1000 pairs for contrastive learning\n",
      "\n",
      "Example pair:\n",
      "Human code: def save_file(filename, data, mk_parents=True):\n",
      "    \"\"\"Save file to disk.\n",
      "    Paramaters\n",
      "    -------...\n",
      "AI code: import pathlib\n",
      "\n",
      "def save_to_disk(filename: pathlib.Path, data: str, mk_parents: bool = False) -> Non...\n"
     ]
    }
   ],
   "source": [
    "# Load the small dataset\n",
    "df_small = pd.read_json('../data/test_dataset_small.jsonl', lines=True)\n",
    "\n",
    "# Create triplets\n",
    "triplets = []\n",
    "\n",
    "for _, row in df_small.iterrows():\n",
    "    code = row['code']\n",
    "    contrast = row['contrast']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Store the pair with labels\n",
    "    if label == 0:\n",
    "        # code is human, contrast is AI\n",
    "        triplets.append({\n",
    "            'human': code,\n",
    "            'ai': contrast\n",
    "        })\n",
    "    else:\n",
    "        # code is AI, contrast is human\n",
    "        triplets.append({\n",
    "            'human': contrast,\n",
    "            'ai': code\n",
    "        })\n",
    "\n",
    "print(f\"✓ Created {len(triplets)} pairs for contrastive learning\")\n",
    "print(\"\\nExample pair:\")\n",
    "print(f\"Human code: {triplets[0]['human'][:100]}...\")\n",
    "print(f\"AI code: {triplets[0]['ai'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa94abcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContrastiveClassifier(\n",
      "  (projection): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total parameters: 427138\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256):\n",
    "        super(ContrastiveClassifier, self).__init__()\n",
    "        # Projection head for contrastive learning\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 128)  # Project to 128-dim space\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_projection=False):\n",
    "        if return_projection:\n",
    "            return self.projection(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Create model\n",
    "contrastive_model = ContrastiveClassifier()\n",
    "print(contrastive_model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in contrastive_model.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71f779c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Contrastive loss function created\n"
     ]
    }
   ],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, human_proj, ai_proj):\n",
    "        # Normalize embeddings\n",
    "        human_proj = F.normalize(human_proj, dim=1)\n",
    "        ai_proj = F.normalize(ai_proj, dim=1)\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity = torch.matmul(human_proj, ai_proj.T) / self.temperature\n",
    "        \n",
    "        # Labels: diagonal elements are positive pairs\n",
    "        batch_size = human_proj.shape[0]\n",
    "        labels = torch.arange(batch_size).to(human_proj.device)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = F.cross_entropy(similarity, labels)\n",
    "        return loss\n",
    "\n",
    "contrastive_criterion = ContrastiveLoss(temperature=0.5)\n",
    "print(\"✓ Contrastive loss function created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a19aa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:31<00:00,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset ready: 1000 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PairedCodeDataset(Dataset):\n",
    "    def __init__(self, triplets, tokenizer, unixcoder_model, max_length=256):\n",
    "        self.triplets = triplets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.unixcoder = unixcoder_model  # This should be UniXcoder\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Pre-compute embeddings for speed\n",
    "        print(\"Pre-computing embeddings...\")\n",
    "        self.human_embeddings = []\n",
    "        self.ai_embeddings = []\n",
    "        \n",
    "        self.unixcoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for pair in tqdm(triplets):\n",
    "                # Get human code embedding\n",
    "                h_inputs = tokenizer(pair['human'], padding='max_length', \n",
    "                                    truncation=True, max_length=max_length, \n",
    "                                    return_tensors=\"pt\")\n",
    "                h_output = self.unixcoder(**h_inputs)\n",
    "                h_emb = h_output.last_hidden_state[:, 0, :].squeeze()\n",
    "                \n",
    "                # Get AI code embedding\n",
    "                a_inputs = tokenizer(pair['ai'], padding='max_length', \n",
    "                                    truncation=True, max_length=max_length, \n",
    "                                    return_tensors=\"pt\")\n",
    "                a_output = self.unixcoder(**a_inputs)\n",
    "                a_emb = a_output.last_hidden_state[:, 0, :].squeeze()\n",
    "                \n",
    "                self.human_embeddings.append(h_emb)\n",
    "                self.ai_embeddings.append(a_emb)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.human_embeddings[idx], self.ai_embeddings[idx]\n",
    "\n",
    "# Create dataset - PASS THE CORRECT MODEL\n",
    "# 'model' should be your UniXcoder model (the one you loaded earlier)\n",
    "paired_dataset = PairedCodeDataset(triplets, tokenizer, model, max_length=256)\n",
    "paired_loader = DataLoader(paired_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"✓ Dataset ready: {len(paired_dataset)} pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0020700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
